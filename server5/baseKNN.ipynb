{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu113'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = '../../arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "def get_data():\n",
    "    with open(FILE) as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: dataframe에 JSON LOAD Columns 모두 추가해서 csv 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = {\n",
    "    'id': [],\n",
    "    'title': [],\n",
    "    'year': [],\n",
    "    'abstract': [],\n",
    "    'author': []\n",
    "}\n",
    "\n",
    "data = get_data()\n",
    "for i, paper in enumerate(data):\n",
    "    paper = json.loads(paper)\n",
    "    # print(paper)\n",
    "    # break\n",
    "    try:\n",
    "        date = int(paper['update_date'].split('-')[0])\n",
    "        if date > 2019:\n",
    "            dataframe['id'].append(paper['id'])\n",
    "            dataframe['title'].append(paper['title'])\n",
    "            dataframe['year'].append(date)\n",
    "            dataframe['abstract'].append(paper['abstract'])\n",
    "            dataframe['author'].append(paper['submitter'])\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 너무 많음 -> cs 관련?\n",
    "인용수 -> 적은 애들 neg sample로 사용 가능\n",
    "\n",
    "graph -> 인접행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0033</td>\n",
       "      <td>Convergence of the discrete dipole approximati...</td>\n",
       "      <td>2022</td>\n",
       "      <td>We performed a rigorous theoretical converge...</td>\n",
       "      <td>Maxim A. Yurkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0038</td>\n",
       "      <td>The discrete dipole approximation: an overview...</td>\n",
       "      <td>2022</td>\n",
       "      <td>We present a review of the discrete dipole a...</td>\n",
       "      <td>Maxim A. Yurkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0275</td>\n",
       "      <td>Mapping radii of metric spaces</td>\n",
       "      <td>2021</td>\n",
       "      <td>It is known that every closed curve of lengt...</td>\n",
       "      <td>George M. Bergman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0348</td>\n",
       "      <td>What can emission lines tell us?</td>\n",
       "      <td>2023</td>\n",
       "      <td>1 Generalities\\n  2 Empirical diagnostics ba...</td>\n",
       "      <td>Grazyna Stasinska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0479</td>\n",
       "      <td>The affine part of the Picard scheme</td>\n",
       "      <td>2021</td>\n",
       "      <td>We describe the maximal torus and maximal un...</td>\n",
       "      <td>Thomas Geisser H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886395</th>\n",
       "      <td>solv-int/9502003</td>\n",
       "      <td>The Hamiltonian structure of the dispersionles...</td>\n",
       "      <td>2020</td>\n",
       "      <td>The Hamiltonian structure of the two-dimensi...</td>\n",
       "      <td>David Fairlie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886396</th>\n",
       "      <td>solv-int/9807001</td>\n",
       "      <td>DNA Transcription Mechanism with a Moving Enzyme</td>\n",
       "      <td>2021</td>\n",
       "      <td>Previous numerical investigations of an one-...</td>\n",
       "      <td>Juhi-Lian Julian Ting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886397</th>\n",
       "      <td>solv-int/9807004</td>\n",
       "      <td>Degenerate Frobenius manifolds and the bi-Hami...</td>\n",
       "      <td>2020</td>\n",
       "      <td>The bi-Hamiltonian structure of certain mult...</td>\n",
       "      <td>I. A. B. Strachan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886398</th>\n",
       "      <td>solv-int/9808019</td>\n",
       "      <td>Hypercomplex Integrable Systems</td>\n",
       "      <td>2020</td>\n",
       "      <td>In this paper we study hypercomplex manifold...</td>\n",
       "      <td>James D. E. Grant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886399</th>\n",
       "      <td>supr-con/9508005</td>\n",
       "      <td>Nonlinear Flux Diffusion and ac Susceptibility...</td>\n",
       "      <td>2022</td>\n",
       "      <td>The ac response of a slab of material with e...</td>\n",
       "      <td>Zbigniew J. Koziol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                              title  \\\n",
       "0              0704.0033  Convergence of the discrete dipole approximati...   \n",
       "1              0704.0038  The discrete dipole approximation: an overview...   \n",
       "2              0704.0275                     Mapping radii of metric spaces   \n",
       "3              0704.0348                   What can emission lines tell us?   \n",
       "4              0704.0479               The affine part of the Picard scheme   \n",
       "...                  ...                                                ...   \n",
       "886395  solv-int/9502003  The Hamiltonian structure of the dispersionles...   \n",
       "886396  solv-int/9807001   DNA Transcription Mechanism with a Moving Enzyme   \n",
       "886397  solv-int/9807004  Degenerate Frobenius manifolds and the bi-Hami...   \n",
       "886398  solv-int/9808019                    Hypercomplex Integrable Systems   \n",
       "886399  supr-con/9508005  Nonlinear Flux Diffusion and ac Susceptibility...   \n",
       "\n",
       "        year                                           abstract  \\\n",
       "0       2022    We performed a rigorous theoretical converge...   \n",
       "1       2022    We present a review of the discrete dipole a...   \n",
       "2       2021    It is known that every closed curve of lengt...   \n",
       "3       2023    1 Generalities\\n  2 Empirical diagnostics ba...   \n",
       "4       2021    We describe the maximal torus and maximal un...   \n",
       "...      ...                                                ...   \n",
       "886395  2020    The Hamiltonian structure of the two-dimensi...   \n",
       "886396  2021    Previous numerical investigations of an one-...   \n",
       "886397  2020    The bi-Hamiltonian structure of certain mult...   \n",
       "886398  2020    In this paper we study hypercomplex manifold...   \n",
       "886399  2022    The ac response of a slab of material with e...   \n",
       "\n",
       "                       author  \n",
       "0             Maxim A. Yurkin  \n",
       "1             Maxim A. Yurkin  \n",
       "2           George M. Bergman  \n",
       "3           Grazyna Stasinska  \n",
       "4            Thomas Geisser H  \n",
       "...                       ...  \n",
       "886395          David Fairlie  \n",
       "886396  Juhi-Lian Julian Ting  \n",
       "886397      I. A. B. Strachan  \n",
       "886398      James D. E. Grant  \n",
       "886399     Zbigniew J. Koziol  \n",
       "\n",
       "[886400 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228644</th>\n",
       "      <td>2009.14794</td>\n",
       "      <td>Rethinking Attention with Performers</td>\n",
       "      <td>2022</td>\n",
       "      <td>We introduce Performers, Transformer archite...</td>\n",
       "      <td>Valerii Likhosherstov</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                 title  year  \\\n",
       "228644  2009.14794  Rethinking Attention with Performers  2022   \n",
       "\n",
       "                                                 abstract  \\\n",
       "228644    We introduce Performers, Transformer archite...   \n",
       "\n",
       "                       author  \n",
       "228644  Valerii Likhosherstov  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['title'] == 'Rethinking Attention with Performers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention is all you need 16667\n",
    "CCNet: Criss-Cross Attention for Semantic Segmentation 36734\n",
    "Heterogeneous Graph Attention Network 45683\n",
    "DeBERTa: Decoding-enhanced BERT with Disentangled Attention 170515\n",
    "Rethinking Attention with Performers 228644\n",
    "\n",
    "[16667, 36734, 45683, 170515, 228644]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5089"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(i) for i in df.abstract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5353c6bcb7654625a758fd42e941d72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing colbert_linear and sparse_linear---------\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 73867/73867 [1:57:03<00:00, 10.52it/s]  \n"
     ]
    }
   ],
   "source": [
    "# 두 시간 걸리니 참고만 하셈\n",
    "output = model.encode(list(df.abstract),\n",
    "                      batch_size=12, \n",
    "                      max_length=5090, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                      )['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df.to_csv('embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv('../../embedding.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=6)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=6)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = KNeighborsClassifier(n_neighbors=6)\n",
    "nn.fit(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨처음 시나리오 있으면 좋겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "Attention Is All You Need\n",
      "\n",
      "Recommendation author-0:\n",
      "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine\n",
      "  Translation\n",
      "\n",
      "Recommendation author-1:\n",
      "Transformer++\n",
      "\n",
      "Recommendation author-2:\n",
      "An Efficient Transformer Decoder with Compressed Sub-layers\n",
      "\n",
      "Recommendation author-3:\n",
      "Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural\n",
      "  Machine Translation\n",
      "\n",
      "Recommendation author-4:\n",
      "A Cost-Efficient FPGA Implementation of Tiny Transformer Model using\n",
      "  Neural ODE\n",
      "\n",
      "===============\n",
      "\n",
      "Sample:\n",
      "CCNet: Criss-Cross Attention for Semantic Segmentation\n",
      "\n",
      "Recommendation author-0:\n",
      "CTNet: Context-based Tandem Network for Semantic Segmentation\n",
      "\n",
      "Recommendation author-1:\n",
      "TriangleNet: Edge Prior Augmented Network for Semantic Segmentation\n",
      "  through Cross-Task Consistency\n",
      "\n",
      "Recommendation author-2:\n",
      "Real-time Semantic Segmentation via Spatial-detail Guided Context\n",
      "  Propagation\n",
      "\n",
      "Recommendation author-3:\n",
      "BiCANet: Bi-directional Contextual Aggregating Network for Image\n",
      "  Semantic Segmentation\n",
      "\n",
      "Recommendation author-4:\n",
      "HCNet: Hierarchical Context Network for Semantic Segmentation\n",
      "\n",
      "===============\n",
      "\n",
      "Sample:\n",
      "Heterogeneous Graph Attention Network\n",
      "\n",
      "Recommendation author-0:\n",
      "SHGNN: Structure-Aware Heterogeneous Graph Neural Network\n",
      "\n",
      "Recommendation author-1:\n",
      "ISHNE: Influence Self-attention for Heterogeneous Network Embedding\n",
      "\n",
      "Recommendation author-2:\n",
      "Simple and Efficient Heterogeneous Graph Neural Network\n",
      "\n",
      "Recommendation author-3:\n",
      "Self-supervised Auxiliary Learning with Meta-paths for Heterogeneous\n",
      "  Graphs\n",
      "\n",
      "Recommendation author-4:\n",
      "SGAT: Simplicial Graph Attention Network\n",
      "\n",
      "===============\n",
      "\n",
      "Sample:\n",
      "DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n",
      "\n",
      "Recommendation author-0:\n",
      "DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in\n",
      "  BERT pretraining\n",
      "\n",
      "Recommendation author-1:\n",
      "DecBERT: Enhancing the Language Understanding of BERT with Causal\n",
      "  Attention Masks\n",
      "\n",
      "Recommendation author-2:\n",
      "Optimizing small BERTs trained for German NER\n",
      "\n",
      "Recommendation author-3:\n",
      "Data-Efficient French Language Modeling with CamemBERTa\n",
      "\n",
      "Recommendation author-4:\n",
      "What the [MASK]? Making Sense of Language-Specific BERT Models\n",
      "\n",
      "===============\n",
      "\n",
      "Sample:\n",
      "Rethinking Attention with Performers\n",
      "\n",
      "Recommendation author-0:\n",
      "Masked Language Modeling for Proteins via Linearly Scalable Long-Context\n",
      "  Transformers\n",
      "\n",
      "Recommendation author-1:\n",
      "cosFormer: Rethinking Softmax in Attention\n",
      "\n",
      "Recommendation author-2:\n",
      "Random Feature Attention\n",
      "\n",
      "Recommendation author-3:\n",
      "Superiority of Softmax: Unveiling the Performance Edge Over Linear\n",
      "  Attention\n",
      "\n",
      "Recommendation author-4:\n",
      "An Attention Matrix for Every Decision: Faithfulness-based Arbitration\n",
      "  Among Multiple Attention-Based Interpretations of Transformers in Text\n",
      "  Classification\n",
      "\n",
      "===============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {i:[] for i in df.columns}\n",
    "for idx in [16667, 36734, 45683, 170515, 228644]:\n",
    "    sample = df[\"title\"][idx]\n",
    "    for i in df.columns: data[i].append(df[i][idx])\n",
    "    dist, index = nn.kneighbors(X=output[idx,:].reshape(1,-1))\n",
    "    print(f\"Sample:\\n{sample}\\n\")\n",
    "    # for i in range(1,6):\n",
    "    #     print(f\"Recommendation {i}:\\n{df['title'][index[0][i]]}\\n\")\n",
    "    #     for j in df.columns: data[j].append(df[j][index[0][i]])\n",
    "    for k in range(1,6):\n",
    "        print(f\"Recommendation {i}-{k-1}:\\n{df['title'][index[0][k]]}\\n\")\n",
    "        for j in df.columns: data[j].append(df[j][index[0][k]])\n",
    "\n",
    "    print(\"===============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {\n",
    "    'nodes': [\n",
    "            {\n",
    "                'title': data['title'][i], \n",
    "                'author': data['author'][i],\n",
    "                'year': int(data['year'][i]),\n",
    "                'id': data['id'][i],\n",
    "                'height': 2 if i==0 else 1,\n",
    "                'size': random.randint(10,20), # citation num\n",
    "                'color': f'rgb(240, 237, 207, {(data[\"year\"][i] - 2015) / 20})' # publish year\n",
    "            }\n",
    "        for i in range(30)], \n",
    "    'links': [\n",
    "            {\n",
    "                'source': data['id'][i],\n",
    "                'target': data['id'][j],\n",
    "                'distance': int(random.randrange(1, 100)) # 100 * similarity\n",
    "            }\n",
    "        for i in range(0, 30, 6) for j in range(i+1, i+6)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "for i in graph['nodes']:\n",
    "    if iter % 6 == 0:\n",
    "        i['size'] = 80\n",
    "        i['color'] = f'rgb(11, 96, 176, 0.8)'\n",
    "    iter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['1706.03762',\n",
       "  '2002.10260',\n",
       "  '2003.04974',\n",
       "  '2101.00542',\n",
       "  '2101.04030',\n",
       "  '2401.02721',\n",
       "  '1811.11721',\n",
       "  '2104.09805',\n",
       "  '2210.05152',\n",
       "  '2005.11034',\n",
       "  '2003.09669',\n",
       "  '2010.04962',\n",
       "  '1903.07293',\n",
       "  '2112.06244',\n",
       "  '2103.00118',\n",
       "  '2207.02547',\n",
       "  '2007.08294',\n",
       "  '2207.11761',\n",
       "  '2006.03654',\n",
       "  '2401.15861',\n",
       "  '2204.08688',\n",
       "  '2104.11559',\n",
       "  '2306.01497',\n",
       "  '2003.02912',\n",
       "  '2009.14794',\n",
       "  '2006.03555',\n",
       "  '2202.08791',\n",
       "  '2103.02143',\n",
       "  '2310.11685',\n",
       "  '2209.10876'],\n",
       " 'title': ['Attention Is All You Need',\n",
       "  'Fixed Encoder Self-Attention Patterns in Transformer-Based Machine\\n  Translation',\n",
       "  'Transformer++',\n",
       "  'An Efficient Transformer Decoder with Compressed Sub-layers',\n",
       "  'Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural\\n  Machine Translation',\n",
       "  'A Cost-Efficient FPGA Implementation of Tiny Transformer Model using\\n  Neural ODE',\n",
       "  'CCNet: Criss-Cross Attention for Semantic Segmentation',\n",
       "  'CTNet: Context-based Tandem Network for Semantic Segmentation',\n",
       "  'TriangleNet: Edge Prior Augmented Network for Semantic Segmentation\\n  through Cross-Task Consistency',\n",
       "  'Real-time Semantic Segmentation via Spatial-detail Guided Context\\n  Propagation',\n",
       "  'BiCANet: Bi-directional Contextual Aggregating Network for Image\\n  Semantic Segmentation',\n",
       "  'HCNet: Hierarchical Context Network for Semantic Segmentation',\n",
       "  'Heterogeneous Graph Attention Network',\n",
       "  'SHGNN: Structure-Aware Heterogeneous Graph Neural Network',\n",
       "  'ISHNE: Influence Self-attention for Heterogeneous Network Embedding',\n",
       "  'Simple and Efficient Heterogeneous Graph Neural Network',\n",
       "  'Self-supervised Auxiliary Learning with Meta-paths for Heterogeneous\\n  Graphs',\n",
       "  'SGAT: Simplicial Graph Attention Network',\n",
       "  'DeBERTa: Decoding-enhanced BERT with Disentangled Attention',\n",
       "  'DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in\\n  BERT pretraining',\n",
       "  'DecBERT: Enhancing the Language Understanding of BERT with Causal\\n  Attention Masks',\n",
       "  'Optimizing small BERTs trained for German NER',\n",
       "  'Data-Efficient French Language Modeling with CamemBERTa',\n",
       "  'What the [MASK]? Making Sense of Language-Specific BERT Models',\n",
       "  'Rethinking Attention with Performers',\n",
       "  'Masked Language Modeling for Proteins via Linearly Scalable Long-Context\\n  Transformers',\n",
       "  'cosFormer: Rethinking Softmax in Attention',\n",
       "  'Random Feature Attention',\n",
       "  'Superiority of Softmax: Unveiling the Performance Edge Over Linear\\n  Attention',\n",
       "  'An Attention Matrix for Every Decision: Faithfulness-based Arbitration\\n  Among Multiple Attention-Based Interpretations of Transformers in Text\\n  Classification'],\n",
       " 'year': [2023,\n",
       "  2020,\n",
       "  2020,\n",
       "  2023,\n",
       "  2021,\n",
       "  2024,\n",
       "  2020,\n",
       "  2021,\n",
       "  2023,\n",
       "  2022,\n",
       "  2020,\n",
       "  2020,\n",
       "  2021,\n",
       "  2021,\n",
       "  2022,\n",
       "  2023,\n",
       "  2021,\n",
       "  2022,\n",
       "  2021,\n",
       "  2024,\n",
       "  2022,\n",
       "  2021,\n",
       "  2023,\n",
       "  2020,\n",
       "  2022,\n",
       "  2020,\n",
       "  2022,\n",
       "  2021,\n",
       "  2023,\n",
       "  2022],\n",
       " 'abstract': ['  The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.\\n',\n",
       "  '  Transformer-based models have brought a radical change to neural machine\\ntranslation. A key feature of the Transformer architecture is the so-called\\nmulti-head attention mechanism, which allows the model to focus simultaneously\\non different parts of the input. However, recent works have shown that most\\nattention heads learn simple, and often redundant, positional patterns. In this\\npaper, we propose to replace all but one attention head of each encoder layer\\nwith simple fixed -- non-learnable -- attentive patterns that are solely based\\non position and do not require any external knowledge. Our experiments with\\ndifferent data sizes and multiple language pairs show that fixing the attention\\nheads on the encoder side of the Transformer at training time does not impact\\nthe translation quality and even increases BLEU scores by up to 3 points in\\nlow-resource scenarios.\\n',\n",
       "  '  Recent advancements in attention mechanisms have replaced recurrent neural\\nnetworks and its variants for machine translation tasks. Transformer using\\nattention mechanism solely achieved state-of-the-art results in sequence\\nmodeling. Neural machine translation based on the attention mechanism is\\nparallelizable and addresses the problem of handling long-range dependencies\\namong words in sentences more effectively than recurrent neural networks. One\\nof the key concepts in attention is to learn three matrices, query, key, and\\nvalue, where global dependencies among words are learned through linearly\\nprojecting word embeddings through these matrices. Multiple query, key, value\\nmatrices can be learned simultaneously focusing on a different subspace of the\\nembedded dimension, which is called multi-head in Transformer. We argue that\\ncertain dependencies among words could be learned better through an\\nintermediate context than directly modeling word-word dependencies. This could\\nhappen due to the nature of certain dependencies or lack of patterns that lend\\nthem difficult to be modeled globally using multi-head self-attention. In this\\nwork, we propose a new way of learning dependencies through a context in\\nmulti-head using convolution. This new form of multi-head attention along with\\nthe traditional form achieves better results than Transformer on the WMT 2014\\nEnglish-to-German and English-to-French translation tasks. We also introduce a\\nframework to learn POS tagging and NER information during the training of\\nencoder which further improves results achieving a new state-of-the-art of 32.1\\nBLEU, better than existing best by 1.4 BLEU, on the WMT 2014 English-to-German\\nand 44.6 BLEU, better than existing best by 1.1 BLEU, on the WMT 2014\\nEnglish-to-French translation tasks. We call this Transformer++.\\n',\n",
       "  '  The large attention-based encoder-decoder network (Transformer) has become\\nprevailing recently due to its effectiveness. But the high computation\\ncomplexity of its decoder raises the inefficiency issue. By examining the\\nmathematic formulation of the decoder, we show that under some mild conditions,\\nthe architecture could be simplified by compressing its sub-layers, the basic\\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\\npropose Compressed Attention Network, whose decoder layer consists of only one\\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\\ntasks show that our model is 1.42x faster with performance on par with a strong\\nbaseline. This strong baseline is already 2x faster than the widely used\\nstandard baseline without loss in performance.\\n',\n",
       "  '  Neural Machine Translation model is a sequence-to-sequence converter based on\\nneural networks. Existing models use recurrent neural networks to construct\\nboth the encoder and decoder modules. In alternative research, the recurrent\\nnetworks were substituted by convolutional neural networks for capturing the\\nsyntactic structure in the input sentence and decreasing the processing time.\\nWe incorporate the goodness of both approaches by proposing a\\nconvolutional-recurrent encoder for capturing the context information as well\\nas the sequential information from the source sentence. Word embedding and\\nposition embedding of the source sentence is performed prior to the\\nconvolutional encoding layer which is basically a n-gram feature extractor\\ncapturing phrase-level context information. The rectified output of the\\nconvolutional encoding layer is added to the original embedding vector, and the\\nsum is normalized by layer normalization. The normalized output is given as a\\nsequential input to the recurrent encoding layer that captures the temporal\\ninformation in the sequence. For the decoder, we use the attention-based\\nrecurrent neural network. Translation task on the German-English dataset\\nverifies the efficacy of the proposed approach from the higher BLEU scores\\nachieved as compared to the state of the art.\\n',\n",
       "  '  Transformer is an emerging neural network model with attention mechanism. It\\nhas been adopted to various tasks and achieved a favorable accuracy compared to\\nCNNs and RNNs. While the attention mechanism is recognized as a general-purpose\\ncomponent, many of the Transformer models require a significant number of\\nparameters compared to the CNN-based ones. To mitigate the computational\\ncomplexity, recently, a hybrid approach has been proposed, which uses ResNet as\\na backbone architecture and replaces a part of its convolution layers with an\\nMHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly\\nreduce the parameter size of such models by using Neural ODE (Ordinary\\nDifferential Equation) as a backbone architecture instead of ResNet. The\\nproposed hybrid model reduces the parameter size by 94.6% compared to the\\nCNN-based ones without degrading the accuracy. We then deploy the proposed\\nmodel on a modest-sized FPGA device for edge computing. To further reduce FPGA\\nresource utilization, we quantize the model following QAT (Quantization Aware\\nTraining) scheme instead of PTQ (Post Training Quantization) to suppress the\\naccuracy loss. As a result, an extremely lightweight Transformer-based model\\ncan be implemented on resource-limited FPGAs. The weights of the feature\\nextraction network are stored on-chip to minimize the memory transfer overhead,\\nallowing faster inference. By eliminating the overhead of memory transfers,\\ninference can be executed seamlessly, leading to accelerated inference. The\\nproposed FPGA implementation achieves 12.8x speedup and 9.21x energy efficiency\\ncompared to ARM Cortex-A53 CPU.\\n',\n",
       "  '  Contextual information is vital in visual understanding problems, such as\\nsemantic segmentation and object detection. We propose a Criss-Cross Network\\n(CCNet) for obtaining full-image contextual information in a very effective and\\nefficient way. Concretely, for each pixel, a novel criss-cross attention module\\nharvests the contextual information of all the pixels on its criss-cross path.\\nBy taking a further recurrent operation, each pixel can finally capture the\\nfull-image dependencies. Besides, a category consistent loss is proposed to\\nenforce the criss-cross attention module to produce more discriminative\\nfeatures. Overall, CCNet is with the following merits: 1) GPU memory friendly.\\nCompared with the non-local block, the proposed recurrent criss-cross attention\\nmodule requires 11x less GPU memory usage. 2) High computational efficiency.\\nThe recurrent criss-cross attention significantly reduces FLOPs by about 85% of\\nthe non-local block. 3) The state-of-the-art performance. We conduct extensive\\nexperiments on semantic segmentation benchmarks including Cityscapes, ADE20K,\\nhuman parsing benchmark LIP, instance segmentation benchmark COCO, video\\nsegmentation benchmark CamVid. In particular, our CCNet achieves the mIoU\\nscores of 81.9%, 45.76% and 55.47% on the Cityscapes test set, the ADE20K\\nvalidation set and the LIP validation set respectively, which are the new\\nstate-of-the-art results. The source codes are available at\\n\\\\url{https://github.com/speedinghzl/CCNet}.\\n',\n",
       "  '  Contextual information has been shown to be powerful for semantic\\nsegmentation. This work proposes a novel Context-based Tandem Network (CTNet)\\nby interactively exploring the spatial contextual information and the channel\\ncontextual information, which can discover the semantic context for semantic\\nsegmentation. Specifically, the Spatial Contextual Module (SCM) is leveraged to\\nuncover the spatial contextual dependency between pixels by exploring the\\ncorrelation between pixels and categories. Meanwhile, the Channel Contextual\\nModule (CCM) is introduced to learn the semantic features including the\\nsemantic feature maps and class-specific features by modeling the long-term\\nsemantic dependence between channels. The learned semantic features are\\nutilized as the prior knowledge to guide the learning of SCM, which can make\\nSCM obtain more accurate long-range spatial dependency. Finally, to further\\nimprove the performance of the learned representations for semantic\\nsegmentation, the results of the two context modules are adaptively integrated\\nto achieve better results. Extensive experiments are conducted on three\\nwidely-used datasets, i.e., PASCAL-Context, ADE20K and PASCAL VOC2012. The\\nresults demonstrate the superior performance of the proposed CTNet by\\ncomparison with several state-of-the-art methods.\\n',\n",
       "  '  This paper addresses the task of semantic segmentation in computer vision,\\naiming to achieve precise pixel-wise classification. We investigate the joint\\ntraining of models for semantic edge detection and semantic segmentation, which\\nhas shown promise. However, implicit cross-task consistency learning in\\nmulti-task networks is limited. To address this, we propose a novel \"decoupled\\ncross-task consistency loss\" that explicitly enhances cross-task consistency.\\nOur semantic segmentation network, TriangleNet, achieves a substantial 2.88\\\\%\\nimprovement over the Baseline in mean Intersection over Union (mIoU) on the\\nCityscapes test set. Notably, TriangleNet operates at 77.4\\\\% mIoU/46.2 FPS on\\nCityscapes, showcasing real-time inference capabilities at full resolution.\\nWith multi-scale inference, performance is further enhanced to 77.8\\\\%.\\nFurthermore, TriangleNet consistently outperforms the Baseline on the FloodNet\\ndataset, demonstrating its robust generalization capabilities. The proposed\\nmethod underscores the significance of multi-task learning and explicit\\ncross-task consistency enhancement for advancing semantic segmentation and\\nhighlights the potential of multitasking in real-time semantic segmentation.\\n',\n",
       "  '  Nowadays, vision-based computing tasks play an important role in various\\nreal-world applications. However, many vision computing tasks, e.g. semantic\\nsegmentation, are usually computationally expensive, posing a challenge to the\\ncomputing systems that are resource-constrained but require fast response\\nspeed. Therefore, it is valuable to develop accurate and real-time vision\\nprocessing models that only require limited computational resources. To this\\nend, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)\\nfor achieving real-time semantic segmentation. In SGCPNet, we propose the\\nstrategy of spatial-detail guided context propagation. It uses the spatial\\ndetails of shallow layers to guide the propagation of the low-resolution global\\ncontexts, in which the lost spatial information can be effectively\\nreconstructed. In this way, the need for maintaining high-resolution features\\nalong the network is freed, therefore largely improving the model efficiency.\\nOn the other hand, due to the effective reconstruction of spatial details, the\\nsegmentation accuracy can be still preserved. In the experiments, we validate\\nthe effectiveness and efficiency of the proposed SGCPNet model. On the\\nCitysacpes dataset, for example, our SGCPNet achieves 69.5% mIoU segmentation\\naccuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX\\n1080 Ti GPU card. In addition, SGCPNet is very lightweight and only contains\\n0.61 M parameters.\\n',\n",
       "  '  Exploring contextual information in convolution neural networks (CNNs) has\\ngained substantial attention in recent years for semantic segmentation. This\\npaper introduces a Bi-directional Contextual Aggregating Network, called\\nBiCANet, for semantic segmentation. Unlike previous approaches that encode\\ncontext in feature space, BiCANet aggregates contextual cues from a categorical\\nperspective, which is mainly consist of three parts: contextual condensed\\nprojection block (CCPB), bi-directional context interaction block (BCIB), and\\nmuti-scale contextual fusion block (MCFB). More specifically, CCPB learns a\\ncategory-based mapping through a split-transform-merge architecture, which\\ncondenses contextual cues with different receptive fields from intermediate\\nlayer. BCIB, on the other hand, employs dense skipped-connections to enhance\\nthe class-level context exchanging. Finally, MCFB integrates multi-scale\\ncontextual cues by investigating short- and long-ranged spatial dependencies.\\nTo evaluate BiCANet, we have conducted extensive experiments on three semantic\\nsegmentation datasets: PASCAL VOC 2012, Cityscapes, and ADE20K. The\\nexperimental results demonstrate that BiCANet outperforms recent\\nstate-of-the-art networks without any postprocess techniques. Particularly,\\nBiCANet achieves the mIoU score of 86.7%, 82.4% and 38.66% on PASCAL VOC 2012,\\nCityscapes and ADE20K testset, respectively.\\n',\n",
       "  '  Global context information is vital in visual understanding problems,\\nespecially in pixel-level semantic segmentation. The mainstream methods adopt\\nthe self-attention mechanism to model global context information. However,\\npixels belonging to different classes usually have weak feature correlation.\\nModeling the global pixel-level correlation matrix indiscriminately is\\nextremely redundant in the self-attention mechanism. In order to solve the\\nabove problem, we propose a hierarchical context network to differentially\\nmodel homogeneous pixels with strong correlations and heterogeneous pixels with\\nweak correlations. Specifically, we first propose a multi-scale guided\\npre-segmentation module to divide the entire feature map into different\\nclassed-based homogeneous regions. Within each homogeneous region, we design\\nthe pixel context module to capture pixel-level correlations. Subsequently,\\ndifferent from the self-attention mechanism that still models weak\\nheterogeneous correlations in a dense pixel-level manner, the region context\\nmodule is proposed to model sparse region-level dependencies using a unified\\nrepresentation of each region. Through aggregating fine-grained pixel context\\nfeatures and coarse-grained region context features, our proposed network can\\nnot only hierarchically model global context information but also harvest\\nmulti-granularity representations to more robustly identify multi-scale\\nobjects. We evaluate our approach on Cityscapes and the ISPRS Vaihingen\\ndataset. Without Bells or Whistles, our approach realizes a mean IoU of 82.8%\\nand overall accuracy of 91.4% on Cityscapes and ISPRS Vaihingen test set,\\nachieving state-of-the-art results.\\n',\n",
       "  '  Graph neural network, as a powerful graph representation technique based on\\ndeep learning, has shown superior performance and attracted considerable\\nresearch interest. However, it has not been fully considered in graph neural\\nnetwork for heterogeneous graph which contains different types of nodes and\\nlinks. The heterogeneity and rich semantic information bring great challenges\\nfor designing a graph neural network for heterogeneous graph. Recently, one of\\nthe most exciting advancements in deep learning is the attention mechanism,\\nwhose great potential has been well demonstrated in various areas. In this\\npaper, we first propose a novel heterogeneous graph neural network based on the\\nhierarchical attention, including node-level and semantic-level attentions.\\nSpecifically, the node-level attention aims to learn the importance between a\\nnode and its metapath based neighbors, while the semantic-level attention is\\nable to learn the importance of different meta-paths. With the learned\\nimportance from both node-level and semantic-level attention, the importance of\\nnode and meta-path can be fully considered. Then the proposed model can\\ngenerate node embedding by aggregating features from meta-path based neighbors\\nin a hierarchical manner. Extensive experimental results on three real-world\\nheterogeneous graphs not only show the superior performance of our proposed\\nmodel over the state-of-the-arts, but also demonstrate its potentially good\\ninterpretability for graph analysis.\\n',\n",
       "  '  Many real-world graphs (networks) are heterogeneous with different types of\\nnodes and edges. Heterogeneous graph embedding, aiming at learning the\\nlow-dimensional node representations of a heterogeneous graph, is vital for\\nvarious downstream applications. Many meta-path based embedding methods have\\nbeen proposed to learn the semantic information of heterogeneous graphs in\\nrecent years. However, most of the existing techniques overlook the graph\\nstructure information when learning the heterogeneous graph embeddings. This\\npaper proposes a novel Structure-Aware Heterogeneous Graph Neural Network\\n(SHGNN) to address the above limitations. In detail, we first utilize a feature\\npropagation module to capture the local structure information of intermediate\\nnodes in the meta-path. Next, we use a tree-attention aggregator to incorporate\\nthe graph structure information into the aggregation module on the meta-path.\\nFinally, we leverage a meta-path aggregator to fuse the information aggregated\\nfrom different meta-paths. We conducted experiments on node classification and\\nclustering tasks and achieved state-of-the-art results on the benchmark\\ndatasets, which shows the effectiveness of our proposed method.\\n',\n",
       "  '  In recent years, Graph Neural Networks has received enormous attention from\\nacademia for its huge potential of modeling the network traits such as\\nmacrostructure and single node attributes. However, prior mainstream works\\nmainly focus on homogeneous network and lack the capacity to characterize the\\nnetwork heterogeneous property. Besides, most previous literature cannot model\\nthe influence under microscope vision, making it infeasible to model the joint\\nrelation between the heterogeneity and mutual interaction within multiple\\nrelation type. In this paper, we propose an Influence Self-attention network to\\naddress the difficulties mentioned above. To model heterogeneity and mutual\\ninteraction, we redesign attention mechanism with influence factor on the\\nsingle-type relation level, which learns the importance coefficient from its\\nadjacent neighbors under the same meta-path based patterns. To incorporate the\\nheterogeneous meta-path in a unified dimension, we developed a self-attention\\nbased framework for meta-path relation fusion according to the learned\\nmeta-path coefficient. Our experimental results demonstrate that our framework\\nnot only achieve higher results than current state-of-the-art baselines, but\\nalso show promising vision on depicting heterogeneous interactive relations\\nunder complicated network structure.\\n',\n",
       "  '  Heterogeneous graph neural networks (HGNNs) have powerful capability to embed\\nrich structural and semantic information of a heterogeneous graph into node\\nrepresentations. Existing HGNNs inherit many mechanisms from graph neural\\nnetworks (GNNs) over homogeneous graphs, especially the attention mechanism and\\nthe multi-layer structure. These mechanisms bring excessive complexity, but\\nseldom work studies whether they are really effective on heterogeneous graphs.\\nThis paper conducts an in-depth and detailed study of these mechanisms and\\nproposes Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN). To\\neasily capture structural information, SeHGNN pre-computes the neighbor\\naggregation using a light-weight mean aggregator, which reduces complexity by\\nremoving overused neighbor attention and avoiding repeated neighbor aggregation\\nin every training epoch. To better utilize semantic information, SeHGNN adopts\\nthe single-layer structure with long metapaths to extend the receptive field,\\nas well as a transformer-based semantic fusion module to fuse features from\\ndifferent metapaths. As a result, SeHGNN exhibits the characteristics of simple\\nnetwork structure, high prediction accuracy, and fast training speed. Extensive\\nexperiments on five real-world heterogeneous graphs demonstrate the superiority\\nof SeHGNN over the state-of-the-arts on both accuracy and training speed.\\n',\n",
       "  '  Graph neural networks have shown superior performance in a wide range of\\napplications providing a powerful representation of graph-structured data.\\nRecent works show that the representation can be further improved by auxiliary\\ntasks. However, the auxiliary tasks for heterogeneous graphs, which contain\\nrich semantic information with various types of nodes and edges, have less\\nexplored in the literature. In this paper, to learn graph neural networks on\\nheterogeneous graphs we propose a novel self-supervised auxiliary learning\\nmethod using meta-paths, which are composite relations of multiple edge types.\\nOur proposed method is learning to learn a primary task by predicting\\nmeta-paths as auxiliary tasks. This can be viewed as a type of meta-learning.\\nThe proposed method can identify an effective combination of auxiliary tasks\\nand automatically balance them to improve the primary task. Our methods can be\\napplied to any graph neural networks in a plug-in manner without manual\\nlabeling or additional data. The experiments demonstrate that the proposed\\nmethod consistently improves the performance of link prediction and node\\nclassification on heterogeneous graphs.\\n',\n",
       "  \"  Heterogeneous graphs have multiple node and edge types and are semantically\\nricher than homogeneous graphs. To learn such complex semantics, many graph\\nneural network approaches for heterogeneous graphs use metapaths to capture\\nmulti-hop interactions between nodes. Typically, features from non-target nodes\\nare not incorporated into the learning procedure. However, there can be\\nnonlinear, high-order interactions involving multiple nodes or edges. In this\\npaper, we present Simplicial Graph Attention Network (SGAT), a simplicial\\ncomplex approach to represent such high-order interactions by placing features\\nfrom non-target nodes on the simplices. We then use attention mechanisms and\\nupper adjacencies to generate representations. We empirically demonstrate the\\nefficacy of our approach with node classification tasks on heterogeneous graph\\ndatasets and further show SGAT's ability in extracting structural information\\nby employing random node features. Numerical experiments indicate that SGAT\\nperforms better than other current state-of-the-art heterogeneous graph\\nlearning methods.\\n\",\n",
       "  \"  Recent progress in pre-trained neural language models has significantly\\nimproved the performance of many natural language processing (NLP) tasks. In\\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\\nwith disentangled attention) that improves the BERT and RoBERTa models using\\ntwo novel techniques. The first is the disentangled attention mechanism, where\\neach word is represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words are computed\\nusing disentangled matrices on their contents and relative positions,\\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\\npositions in the decoding layer to predict the masked tokens in model\\npre-training. In addition, a new virtual adversarial training method is used\\nfor fine-tuning to improve models' generalization. We show that these\\ntechniques significantly improve the efficiency of model pre-training and the\\nperformance of both natural language understanding (NLU) and natural langauge\\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\\ntrained on half of the training data performs consistently better on a wide\\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\\nNotably, we scale up DeBERTa by training a larger version that consists of 48\\nTransform layers with 1.5 billion parameters. The significant performance boost\\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\\nby a decent margin (90.3 versus 89.8).\\n\",\n",
       "  \"  BERT (Bidirectional Encoder Representations from Transformers) has\\nrevolutionized the field of natural language processing through its exceptional\\nperformance on numerous tasks. Yet, the majority of researchers have mainly\\nconcentrated on enhancements related to the model structure, such as relative\\nposition embedding and more efficient attention mechanisms. Others have delved\\ninto pretraining tricks associated with Masked Language Modeling, including\\nwhole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's\\nencoder model for pretraining, proving to be highly effective. We argue that\\nthe design and research around enhanced masked language modeling decoders have\\nbeen underappreciated. In this paper, we propose several designs of enhanced\\ndecoders and introduce DrBERT (Decoder-refined BERT), a novel method for\\nmodeling training. Typically, a pretrained BERT model is fine-tuned for\\nspecific Natural Language Understanding (NLU) tasks. In our approach, we\\nutilize the original BERT model as the encoder, making only changes to the\\ndecoder without altering the encoder. This approach does not necessitate\\nextensive modifications to the model's architecture and can be seamlessly\\nintegrated into existing fine-tuning pipelines and services, offering an\\nefficient and effective enhancement strategy. Compared to other methods, while\\nwe also incur a moderate training cost for the decoder during the pretraining\\nprocess, our approach does not introduce additional training costs during the\\nfine-tuning phase. We test multiple enhanced decoder structures after\\npretraining and evaluate their performance on the GLUE benchmark. Our results\\ndemonstrate that DrBERT, having only undergone subtle refinements to the model\\nstructure during pretraining, significantly enhances model performance without\\nescalating the inference time and serving budget.\\n\",\n",
       "  '  Since 2017, the Transformer-based models play critical roles in various\\ndownstream Natural Language Processing tasks. However, a common limitation of\\nthe attention mechanism utilized in Transformer Encoder is that it cannot\\nautomatically capture the information of word order, so explicit position\\nembeddings are generally required to be fed into the target model. In contrast,\\nTransformer Decoder with the causal attention masks is naturally sensitive to\\nthe word order. In this work, we focus on improving the position encoding\\nability of BERT with the causal attention masks. Furthermore, we propose a new\\npre-trained language model DecBERT and evaluate it on the GLUE benchmark.\\nExperimental results show that (1) the causal attention mask is effective for\\nBERT on the language understanding tasks; (2) our DecBERT model without\\nposition embeddings achieve comparable performance on the GLUE benchmark; and\\n(3) our modification accelerates the pre-training process and DecBERT w/ PE\\nachieves better overall performance than the baseline systems when pre-training\\nwith the same amount of computational resources.\\n',\n",
       "  '  Currently, the most widespread neural network architecture for training\\nlanguage models is the so called BERT which led to improvements in various\\nNatural Language Processing (NLP) tasks. In general, the larger the number of\\nparameters in a BERT model, the better the results obtained in these NLP tasks.\\nUnfortunately, the memory consumption and the training duration drastically\\nincreases with the size of these models. In this article, we investigate\\nvarious training techniques of smaller BERT models: We combine different\\nmethods from other BERT variants like ALBERT, RoBERTa, and relative positional\\nencoding. In addition, we propose two new fine-tuning modifications leading to\\nbetter performance: Class-Start-End tagging and a modified form of Linear Chain\\nConditional Random Fields. Furthermore, we introduce Whole-Word Attention which\\nreduces BERTs memory usage and leads to a small increase in performance\\ncompared to classical Multi-Head-Attention. We evaluate these techniques on\\nfive public German Named Entity Recognition (NER) tasks of which two are\\nintroduced by this article.\\n',\n",
       "  \"  Recent advances in NLP have significantly improved the performance of\\nlanguage models on a variety of tasks. While these advances are largely driven\\nby the availability of large amounts of data and computational power, they also\\nbenefit from the development of better training methods and architectures. In\\nthis paper, we introduce CamemBERTa, a French DeBERTa model that builds upon\\nthe DeBERTaV3 architecture and training objective. We evaluate our model's\\nperformance on a variety of French downstream tasks and datasets, including\\nquestion answering, part-of-speech tagging, dependency parsing, named entity\\nrecognition, and the FLUE benchmark, and compare against CamemBERT, the\\nstate-of-the-art monolingual model for French. Our results show that, given the\\nsame amount of training tokens, our model outperforms BERT-based models trained\\nwith MLM on most tasks. Furthermore, our new model reaches similar or superior\\nperformance on downstream tasks compared to CamemBERT, despite being trained on\\nonly 30% of its total number of input tokens. In addition to our experimental\\nresults, we also publicly release the weights and code implementation of\\nCamemBERTa, making it the first publicly available DeBERTaV3 model outside of\\nthe original paper and the first openly available implementation of a DeBERTaV3\\ntraining objective. https://gitlab.inria.fr/almanach/CamemBERTa\\n\",\n",
       "  '  Recently, Natural Language Processing (NLP) has witnessed an impressive\\nprogress in many areas, due to the advent of novel, pretrained contextual\\nrepresentation models. In particular, Devlin et al. (2019) proposed a model,\\ncalled BERT (Bidirectional Encoder Representations from Transformers), which\\nenables researchers to obtain state-of-the art performance on numerous NLP\\ntasks by fine-tuning the representations on their data set and task, without\\nthe need for developing and training highly-specific architectures. The authors\\nalso released multilingual BERT (mBERT), a model trained on a corpus of 104\\nlanguages, which can serve as a universal language model. This model obtained\\nimpressive results on a zero-shot cross-lingual natural inference task. Driven\\nby the potential of BERT models, the NLP community has started to investigate\\nand generate an abundant number of BERT models that are trained on a particular\\nlanguage, and tested on a specific data domain and task. This allows us to\\nevaluate the true potential of mBERT as a universal language model, by\\ncomparing it to the performance of these more specific models. This paper\\npresents the current state of the art in language-specific BERT models,\\nproviding an overall picture with respect to different dimensions (i.e.\\narchitectures, data domains, and tasks). Our aim is to provide an immediate and\\nstraightforward overview of the commonalities and differences between\\nLanguage-Specific (language-specific) BERT models and mBERT. We also provide an\\ninteractive and constantly updated website that can be used to explore the\\ninformation we have collected, at https://bertlang.unibocconi.it.\\n',\n",
       "  '  We introduce Performers, Transformer architectures which can estimate regular\\n(softmax) full-rank-attention Transformers with provable accuracy, but using\\nonly linear (as opposed to quadratic) space and time complexity, without\\nrelying on any priors such as sparsity or low-rankness. To approximate softmax\\nattention-kernels, Performers use a novel Fast Attention Via positive\\nOrthogonal Random features approach (FAVOR+), which may be of independent\\ninterest for scalable kernel methods. FAVOR+ can be also used to efficiently\\nmodel kernelizable attention mechanisms beyond softmax. This representational\\npower is crucial to accurately compare softmax with other kernels for the first\\ntime on large-scale tasks, beyond the reach of regular Transformers, and\\ninvestigate optimal attention-kernels. Performers are linear architectures\\nfully compatible with regular Transformers and with strong theoretical\\nguarantees: unbiased or nearly-unbiased estimation of the attention matrix,\\nuniform convergence and low estimation variance. We tested Performers on a rich\\nset of tasks stretching from pixel-prediction through text models to protein\\nsequence modeling. We demonstrate competitive results with other examined\\nefficient sparse and dense attention methods, showcasing effectiveness of the\\nnovel attention-learning paradigm leveraged by Performers.\\n',\n",
       "  '  Transformer models have achieved state-of-the-art results across a diverse\\nrange of domains. However, concern over the cost of training the attention\\nmechanism to learn complex dependencies between distant inputs continues to\\ngrow. In response, solutions that exploit the structure and sparsity of the\\nlearned attention matrix have blossomed. However, real-world applications that\\ninvolve long sequences, such as biological sequence analysis, may fall short of\\nmeeting these assumptions, precluding exploration of these models. To address\\nthis challenge, we present a new Transformer architecture, Performer, based on\\nFast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales\\nlinearly rather than quadratically in the number of tokens in the sequence, is\\ncharacterized by sub-quadratic space complexity and does not incorporate any\\nsparsity pattern priors. Furthermore, it provides strong theoretical\\nguarantees: unbiased estimation of the attention matrix and uniform\\nconvergence. It is also backwards-compatible with pre-trained regular\\nTransformers. We demonstrate its effectiveness on the challenging task of\\nprotein sequence modeling and provide detailed theoretical analysis.\\n',\n",
       "  '  Transformer has shown great successes in natural language processing,\\ncomputer vision, and audio processing. As one of its core components, the\\nsoftmax attention helps to capture long-range dependencies yet prohibits its\\nscale-up due to the quadratic space and time complexity to the sequence length.\\nKernel methods are often adopted to reduce the complexity by approximating the\\nsoftmax operator. Nevertheless, due to the approximation errors, their\\nperformances vary in different tasks/corpus and suffer crucial performance\\ndrops when compared with the vanilla softmax attention. In this paper, we\\npropose a linear transformer called cosFormer that can achieve comparable or\\nbetter accuracy to the vanilla transformer in both casual and cross attentions.\\ncosFormer is based on two key properties of softmax attention: i).\\nnon-negativeness of the attention matrix; ii). a non-linear re-weighting scheme\\nthat can concentrate the distribution of the attention matrix. As its linear\\nsubstitute, cosFormer fulfills these properties with a linear operator and a\\ncosine-based distance re-weighting mechanism. Extensive experiments on language\\nmodeling and text understanding tasks demonstrate the effectiveness of our\\nmethod. We further examine our method on long sequences and achieve\\nstate-of-the-art performance on the Long-Range Arena benchmark. The source code\\nis available at https://github.com/OpenNLPLab/cosFormer.\\n',\n",
       "  \"  Transformers are state-of-the-art models for a variety of sequence modeling\\ntasks. At their core is an attention function which models pairwise\\ninteractions between the inputs at every timestep. While attention is powerful,\\nit does not scale efficiently to long sequences due to its quadratic time and\\nspace complexity in the sequence length. We propose RFA, a linear time and\\nspace attention that uses random feature methods to approximate the softmax\\nfunction, and explore its application in transformers. RFA can be used as a\\ndrop-in replacement for conventional softmax attention and offers a\\nstraightforward way of learning with recency bias through an optional gating\\nmechanism. Experiments on language modeling and machine translation demonstrate\\nthat RFA achieves similar or better performance compared to strong transformer\\nbaselines. In the machine translation experiment, RFA decodes twice as fast as\\na vanilla transformer. Compared to existing efficient transformer variants, RFA\\nis competitive in terms of both accuracy and efficiency on three long text\\nclassification datasets. Our analysis shows that RFA's efficiency gains are\\nespecially notable on long sequences, suggesting that RFA will be particularly\\nuseful in tasks that require working with large inputs, fast decoding speed, or\\nlow memory footprints.\\n\",\n",
       "  '  Large transformer models have achieved state-of-the-art results in numerous\\nnatural language processing tasks. Among the pivotal components of the\\ntransformer architecture, the attention mechanism plays a crucial role in\\ncapturing token interactions within sequences through the utilization of\\nsoftmax function.\\n  Conversely, linear attention presents a more computationally efficient\\nalternative by approximating the softmax operation with linear complexity.\\nHowever, it exhibits substantial performance degradation when compared to the\\ntraditional softmax attention mechanism.\\n  In this paper, we bridge the gap in our theoretical understanding of the\\nreasons behind the practical performance gap between softmax and linear\\nattention. By conducting a comprehensive comparative analysis of these two\\nattention mechanisms, we shed light on the underlying reasons for why softmax\\nattention outperforms linear attention in most scenarios.\\n',\n",
       "  '  Transformers are widely used in natural language processing, where they\\nconsistently achieve state-of-the-art performance. This is mainly due to their\\nattention-based architecture, which allows them to model rich linguistic\\nrelations between (sub)words. However, transformers are difficult to interpret.\\nBeing able to provide reasoning for its decisions is an important property for\\na model in domains where human lives are affected. With transformers finding\\nwide use in such fields, the need for interpretability techniques tailored to\\nthem arises. We propose a new technique that selects the most faithful\\nattention-based interpretation among the several ones that can be obtained by\\ncombining different head, layer and matrix operations. In addition, two\\nvariations are introduced towards (i) reducing the computational complexity,\\nthus being faster and friendlier to the environment, and (ii) enhancing the\\nperformance in multi-label data. We further propose a new faithfulness metric\\nthat is more suitable for transformer models and exhibits high correlation with\\nthe area under the precision-recall curve based on ground truth rationales. We\\nvalidate the utility of our contributions with a series of quantitative and\\nqualitative experiments on seven datasets.\\n'],\n",
       " 'author': ['Llion Jones',\n",
       "  'Alessandro Raganato',\n",
       "  'Prakhar Thapak',\n",
       "  'Ye Lin',\n",
       "  'Seba Susan',\n",
       "  'Ikumi Okubo',\n",
       "  'Yunchao Wei',\n",
       "  'Yanpeng Sun',\n",
       "  'Rui Zheng',\n",
       "  'Yuan Zhou',\n",
       "  'Dechun Cong',\n",
       "  'Congchong Nie',\n",
       "  'Houye Ji',\n",
       "  'Wentao Xu',\n",
       "  'Yang Yan',\n",
       "  'Xiaocheng Yang',\n",
       "  'Dasol Hwang',\n",
       "  'See Hian Lee',\n",
       "  'Pengcheng He',\n",
       "  'Wen Liang',\n",
       "  'Ziyang Luo',\n",
       "  'Jochen Z\\\\\"ollner',\n",
       "  'Wissam Antoun',\n",
       "  'Federico Bianchi',\n",
       "  'Valerii Likhosherstov',\n",
       "  'Xingyou Song',\n",
       "  'Yiran Zhong',\n",
       "  'Hao Peng',\n",
       "  'Yichuan Deng',\n",
       "  'Ioannis Mollas']}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "nodes: [\n",
    "    {                   # paper에 대한 정보\n",
    "        id: str -> xxxx.xxxx           # 챗봇 연결링크 생성시 필요, Nullable: False\n",
    "        title: str -> Attention is ~   # 유저한테 보여줄 논문 제목 -> 만드는중\n",
    "        size: int                      # 인용수\n",
    "        hieght: int                    # 부모 노드에 연결될 Edge 굵기: 1hop 2hop 구분으로 사용?\n",
    "        color: int                     # 노드 특성 반영한 한가지 값. 투명도로 조절해서 표시할 수 있음.\n",
    "    }, ...\n",
    "],\n",
    "edges: [\n",
    "    {\n",
    "        source: str -> id           # 자식 노드 제목\n",
    "        to: str -> id               # 부모 노드 제목\n",
    "        distance -> int [0 ~ 1]     # 모델 output 유사도\n",
    "    }, ...\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'title': 'Attention Is All You Need',\n",
       "   'author': 'Llion Jones',\n",
       "   'year': 2023,\n",
       "   'id': '1706.03762',\n",
       "   'height': 2,\n",
       "   'size': 80,\n",
       "   'color': 'rgb(240, 237, 207, 0.8)'},\n",
       "  {'title': 'Fixed Encoder Self-Attention Patterns in Transformer-Based Machine\\n  Translation',\n",
       "   'author': 'Alessandro Raganato',\n",
       "   'year': 2020,\n",
       "   'id': '2002.10260',\n",
       "   'height': 1,\n",
       "   'size': 77,\n",
       "   'color': 'rgb(11, 96, 176, 0.25)'},\n",
       "  {'title': 'Transformer++',\n",
       "   'author': 'Prakhar Thapak',\n",
       "   'year': 2020,\n",
       "   'id': '2003.04974',\n",
       "   'height': 1,\n",
       "   'size': 34,\n",
       "   'color': 'rgb(11, 96, 176, 0.25)'},\n",
       "  {'title': 'An Efficient Transformer Decoder with Compressed Sub-layers',\n",
       "   'author': 'Ye Lin',\n",
       "   'year': 2023,\n",
       "   'id': '2101.00542',\n",
       "   'height': 1,\n",
       "   'size': 43,\n",
       "   'color': 'rgb(11, 96, 176, 0.4)'},\n",
       "  {'title': 'Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural\\n  Machine Translation',\n",
       "   'author': 'Seba Susan',\n",
       "   'year': 2021,\n",
       "   'id': '2101.04030',\n",
       "   'height': 1,\n",
       "   'size': 39,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'A Cost-Efficient FPGA Implementation of Tiny Transformer Model using\\n  Neural ODE',\n",
       "   'author': 'Ikumi Okubo',\n",
       "   'year': 2024,\n",
       "   'id': '2401.02721',\n",
       "   'height': 1,\n",
       "   'size': 80,\n",
       "   'color': 'rgb(240, 237, 207, 0.8)'},\n",
       "  {'title': 'CCNet: Criss-Cross Attention for Semantic Segmentation',\n",
       "   'author': 'Yunchao Wei',\n",
       "   'year': 2020,\n",
       "   'id': '1811.11721',\n",
       "   'height': 1,\n",
       "   'size': 39,\n",
       "   'color': 'rgb(11, 96, 176, 0.25)'},\n",
       "  {'title': 'CTNet: Context-based Tandem Network for Semantic Segmentation',\n",
       "   'author': 'Yanpeng Sun',\n",
       "   'year': 2021,\n",
       "   'id': '2104.09805',\n",
       "   'height': 1,\n",
       "   'size': 30,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'TriangleNet: Edge Prior Augmented Network for Semantic Segmentation\\n  through Cross-Task Consistency',\n",
       "   'author': 'Rui Zheng',\n",
       "   'year': 2023,\n",
       "   'id': '2210.05152',\n",
       "   'height': 1,\n",
       "   'size': 31,\n",
       "   'color': 'rgb(11, 96, 176, 0.4)'},\n",
       "  {'title': 'Real-time Semantic Segmentation via Spatial-detail Guided Context\\n  Propagation',\n",
       "   'author': 'Yuan Zhou',\n",
       "   'year': 2022,\n",
       "   'id': '2005.11034',\n",
       "   'height': 1,\n",
       "   'size': 93,\n",
       "   'color': 'rgb(11, 96, 176, 0.35)'},\n",
       "  {'title': 'BiCANet: Bi-directional Contextual Aggregating Network for Image\\n  Semantic Segmentation',\n",
       "   'author': 'Dechun Cong',\n",
       "   'year': 2020,\n",
       "   'id': '2003.09669',\n",
       "   'height': 1,\n",
       "   'size': 80,\n",
       "   'color': 'rgb(240, 237, 207, 0.8)'},\n",
       "  {'title': 'HCNet: Hierarchical Context Network for Semantic Segmentation',\n",
       "   'author': 'Congchong Nie',\n",
       "   'year': 2020,\n",
       "   'id': '2010.04962',\n",
       "   'height': 1,\n",
       "   'size': 42,\n",
       "   'color': 'rgb(11, 96, 176, 0.25)'},\n",
       "  {'title': 'Heterogeneous Graph Attention Network',\n",
       "   'author': 'Houye Ji',\n",
       "   'year': 2021,\n",
       "   'id': '1903.07293',\n",
       "   'height': 1,\n",
       "   'size': 38,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'SHGNN: Structure-Aware Heterogeneous Graph Neural Network',\n",
       "   'author': 'Wentao Xu',\n",
       "   'year': 2021,\n",
       "   'id': '2112.06244',\n",
       "   'height': 1,\n",
       "   'size': 37,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'ISHNE: Influence Self-attention for Heterogeneous Network Embedding',\n",
       "   'author': 'Yang Yan',\n",
       "   'year': 2022,\n",
       "   'id': '2103.00118',\n",
       "   'height': 1,\n",
       "   'size': 46,\n",
       "   'color': 'rgb(11, 96, 176, 0.35)'},\n",
       "  {'title': 'Simple and Efficient Heterogeneous Graph Neural Network',\n",
       "   'author': 'Xiaocheng Yang',\n",
       "   'year': 2023,\n",
       "   'id': '2207.02547',\n",
       "   'height': 1,\n",
       "   'size': 80,\n",
       "   'color': 'rgb(240, 237, 207, 0.8)'},\n",
       "  {'title': 'Self-supervised Auxiliary Learning with Meta-paths for Heterogeneous\\n  Graphs',\n",
       "   'author': 'Dasol Hwang',\n",
       "   'year': 2021,\n",
       "   'id': '2007.08294',\n",
       "   'height': 1,\n",
       "   'size': 39,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'SGAT: Simplicial Graph Attention Network',\n",
       "   'author': 'See Hian Lee',\n",
       "   'year': 2022,\n",
       "   'id': '2207.11761',\n",
       "   'height': 1,\n",
       "   'size': 40,\n",
       "   'color': 'rgb(11, 96, 176, 0.35)'},\n",
       "  {'title': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention',\n",
       "   'author': 'Pengcheng He',\n",
       "   'year': 2021,\n",
       "   'id': '2006.03654',\n",
       "   'height': 1,\n",
       "   'size': 90,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in\\n  BERT pretraining',\n",
       "   'author': 'Wen Liang',\n",
       "   'year': 2024,\n",
       "   'id': '2401.15861',\n",
       "   'height': 1,\n",
       "   'size': 49,\n",
       "   'color': 'rgb(11, 96, 176, 0.45)'},\n",
       "  {'title': 'DecBERT: Enhancing the Language Understanding of BERT with Causal\\n  Attention Masks',\n",
       "   'author': 'Ziyang Luo',\n",
       "   'year': 2022,\n",
       "   'id': '2204.08688',\n",
       "   'height': 1,\n",
       "   'size': 80,\n",
       "   'color': 'rgb(240, 237, 207, 0.8)'},\n",
       "  {'title': 'Optimizing small BERTs trained for German NER',\n",
       "   'author': 'Jochen Z\\\\\"ollner',\n",
       "   'year': 2021,\n",
       "   'id': '2104.11559',\n",
       "   'height': 1,\n",
       "   'size': 72,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'Data-Efficient French Language Modeling with CamemBERTa',\n",
       "   'author': 'Wissam Antoun',\n",
       "   'year': 2023,\n",
       "   'id': '2306.01497',\n",
       "   'height': 1,\n",
       "   'size': 92,\n",
       "   'color': 'rgb(11, 96, 176, 0.4)'},\n",
       "  {'title': 'What the [MASK]? Making Sense of Language-Specific BERT Models',\n",
       "   'author': 'Federico Bianchi',\n",
       "   'year': 2020,\n",
       "   'id': '2003.02912',\n",
       "   'height': 1,\n",
       "   'size': 52,\n",
       "   'color': 'rgb(11, 96, 176, 0.25)'},\n",
       "  {'title': 'Rethinking Attention with Performers',\n",
       "   'author': 'Valerii Likhosherstov',\n",
       "   'year': 2022,\n",
       "   'id': '2009.14794',\n",
       "   'height': 1,\n",
       "   'size': 47,\n",
       "   'color': 'rgb(11, 96, 176, 0.35)'},\n",
       "  {'title': 'Masked Language Modeling for Proteins via Linearly Scalable Long-Context\\n  Transformers',\n",
       "   'author': 'Xingyou Song',\n",
       "   'year': 2020,\n",
       "   'id': '2006.03555',\n",
       "   'height': 1,\n",
       "   'size': 80,\n",
       "   'color': 'rgb(240, 237, 207, 0.8)'},\n",
       "  {'title': 'cosFormer: Rethinking Softmax in Attention',\n",
       "   'author': 'Yiran Zhong',\n",
       "   'year': 2022,\n",
       "   'id': '2202.08791',\n",
       "   'height': 1,\n",
       "   'size': 89,\n",
       "   'color': 'rgb(11, 96, 176, 0.35)'},\n",
       "  {'title': 'Random Feature Attention',\n",
       "   'author': 'Hao Peng',\n",
       "   'year': 2021,\n",
       "   'id': '2103.02143',\n",
       "   'height': 1,\n",
       "   'size': 56,\n",
       "   'color': 'rgb(11, 96, 176, 0.3)'},\n",
       "  {'title': 'Superiority of Softmax: Unveiling the Performance Edge Over Linear\\n  Attention',\n",
       "   'author': 'Yichuan Deng',\n",
       "   'year': 2023,\n",
       "   'id': '2310.11685',\n",
       "   'height': 1,\n",
       "   'size': 57,\n",
       "   'color': 'rgb(11, 96, 176, 0.4)'},\n",
       "  {'title': 'An Attention Matrix for Every Decision: Faithfulness-based Arbitration\\n  Among Multiple Attention-Based Interpretations of Transformers in Text\\n  Classification',\n",
       "   'author': 'Ioannis Mollas',\n",
       "   'year': 2022,\n",
       "   'id': '2209.10876',\n",
       "   'height': 1,\n",
       "   'size': 51,\n",
       "   'color': 'rgb(11, 96, 176, 0.35)'}],\n",
       " 'links': [{'source': '1706.03762', 'target': '2002.10260', 'distance': 80},\n",
       "  {'source': '1706.03762', 'target': '2003.04974', 'distance': 88},\n",
       "  {'source': '1706.03762', 'target': '2101.00542', 'distance': 22},\n",
       "  {'source': '1706.03762', 'target': '2101.04030', 'distance': 17},\n",
       "  {'source': '1706.03762', 'target': '2401.02721', 'distance': 61},\n",
       "  {'source': '1811.11721', 'target': '2104.09805', 'distance': 92},\n",
       "  {'source': '1811.11721', 'target': '2210.05152', 'distance': 39},\n",
       "  {'source': '1811.11721', 'target': '2005.11034', 'distance': 73},\n",
       "  {'source': '1811.11721', 'target': '2003.09669', 'distance': 21},\n",
       "  {'source': '1811.11721', 'target': '2010.04962', 'distance': 67},\n",
       "  {'source': '1903.07293', 'target': '2112.06244', 'distance': 47},\n",
       "  {'source': '1903.07293', 'target': '2103.00118', 'distance': 48},\n",
       "  {'source': '1903.07293', 'target': '2207.02547', 'distance': 38},\n",
       "  {'source': '1903.07293', 'target': '2007.08294', 'distance': 47},\n",
       "  {'source': '1903.07293', 'target': '2207.11761', 'distance': 22},\n",
       "  {'source': '2006.03654', 'target': '2401.15861', 'distance': 79},\n",
       "  {'source': '2006.03654', 'target': '2204.08688', 'distance': 10},\n",
       "  {'source': '2006.03654', 'target': '2104.11559', 'distance': 94},\n",
       "  {'source': '2006.03654', 'target': '2306.01497', 'distance': 14},\n",
       "  {'source': '2006.03654', 'target': '2003.02912', 'distance': 26},\n",
       "  {'source': '2009.14794', 'target': '2006.03555', 'distance': 81},\n",
       "  {'source': '2009.14794', 'target': '2202.08791', 'distance': 83},\n",
       "  {'source': '2009.14794', 'target': '2103.02143', 'distance': 61},\n",
       "  {'source': '2009.14794', 'target': '2310.11685', 'distance': 9},\n",
       "  {'source': '2009.14794', 'target': '2209.10876', 'distance': 48}]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "with open(os.path.join('back/data', 'attentiongraph.json'), 'w') as json_file:\n",
    "    json.dump(graph, json_file, ensure_ascii=False) # ensure_ascii True 하면 한글이 안보임 ㅋㅋ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
